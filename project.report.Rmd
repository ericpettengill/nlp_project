---
title: "Topic Modelling FDA Recall Text Data"
author: 
- "Eric Pettengill"
- "Jeff Church"
abstract: |
  A study by Zhang et al.[@zhang] examined medical device recalls in the United States from the years 2012-2015 with the goal of determining the impact of software user-interface (UI) errors on patient safety. To identify recalls due to software UI errors, a laborious manual classification process was used. To reduce the manual labor involved in this process, we applied Latent Dirichlet Allocation (LDA) to the task of automatically identifying recalls due to software UI errors.
  
  Using LDA, approximately 96% of software UI recalls were successfully identified. However, due to the similarity between recall descriptions there was also a high number of false positives. Overall accuracy could potentially be improved through further data pre-processing. While the task of automatically identifying software UI recalls was not solved completely, LDA successfully reduced the number of recalls that must be manually classified.
output: 
  pdf_document:
    citation_package: biblatex
bibliography: project.references.bib
---

```{r LIBRARIES, warning=FALSE, include=FALSE, message=FALSE}
library(readxl)
library(tidyverse)
library(tidytext)
library(tm)
library(topicmodels)
# library(SnowballC)
library(broom)
# library(caret)
# library(forcats)
library(kableExtra)
library(ggthemes)
library(patchwork)
```

```{r DATA, message=FALSE, warning=FALSE, include=FALSE}
get_labelled_results <- function(){
  
labeled1 <- suppressMessages(suppressWarnings(read_xls("UI SW Labelled Results.xls", sheet = 1)))
labeled2 <- suppressMessages(read_xls("UI SW Labelled Results.xls", sheet = 2))
labeled3 <- suppressMessages(read_xls("UI SW Labelled Results.xls", sheet = 3))

x1 <- labeled1 %>% 
  select(DATASET) %>% 
  pull() %>% 
  str_split(":", 2, simplify = TRUE) %>% 
  as.data.frame() %>% 
  filter(V1 %in% c("RECALL_NUMBER"))

x2 <- labeled2 %>% 
  select(DATASET) %>% 
  pull() %>% 
  str_split(":", 2, simplify = TRUE) %>% 
  as.data.frame() %>% 
  filter(V1 %in% c("RECALL_NUMBER"))

x3 <- labeled3 %>% 
  select(DATASET) %>% 
  pull() %>% 
  str_split(":", 2, simplify = TRUE) %>% 
  as.data.frame() %>% 
  filter(V1 %in% c("RECALL_NUMBER"))

RECALL_NUMBER <- as.character(rbind(x1, x2, x3)$V2) 

finalx1 <- labeled1 %>% 
  select(`UI-SW`, `CTRL-SW`,  `SW`, `Root Cause`) %>% 
  filter(`UI-SW` == "x" | `CTRL-SW` == "x" | `SW` == "x")

finalx2 <- labeled2 %>% 
  select(`UI-SW`, `CTRL-SW`,  `SW`, `Root Cause`) %>% 
  filter(`UI-SW` == "x" | `CTRL-SW` == "x" | `SW` == "x")

finalx3 <- labeled3 %>% 
  select(`UI-SW`, `CTRL-SW`,  `SW`, `Root Cause` = `Other/Root Cause`) %>% 
  filter(`UI-SW` == "x" | `CTRL-SW` == "x" | `SW` == "x")

final <- bind_rows(finalx1, finalx2, finalx3) %>% 
  add_column(RECALL_NUMBER)

final$RECALL_NUMBER <- trimws(final$RECALL_NUMBER)


get_recalls <- function(){
# loads excel spreadsheets from elearning
recall_1 <- read_xlsx("201209-201308 Recalls with Cause.xlsx", sheet = 1) %>% 
  rename(FDAcause = `FDA Determined Cause`)

recall_2 <- read_xls("201309-201408 Recalls with Causes.xls", sheet = 1) %>% 
  rename(FDAcause = `FDA Determined Cause`)

recall_3 <- read_xlsx("201409-201508 Recalls with Causes.xlsx", sheet = 2) %>% 
  rename(FDAcause = `FDA Determined Causes`)

# combine all 3 spreadsheets
recall <- bind_rows(recall_1, recall_2, recall_3)

return(recall)
}

recall <- get_recalls() 

recall_labelled <- left_join(recall, final, by = "RECALL_NUMBER") %>% 
  rename(UI = `UI-SW`, CTRL = `CTRL-SW`, root_cause = `Root Cause`) %>% 
  mutate(UI = if_else(UI == "x", 1, 0, missing = 0), 
         CTRL = if_else(CTRL == "x", 1, 0, missing = 0),
         SW = if_else(SW == "x", 1, 0, missing = 0))


return(recall_labelled)
}



labeled <- get_labelled_results()
```

```{r DATA PROCESSING, message=FALSE, warning=FALSE, include=FALSE}
fda_causes_rm <- c("Counterfeit", "Error in labeling", "Finished device change control", "Incorrect or no expiration date", "Labeling Change Control", "Labeling False and Misleading
", "Labeling design", "Labeling mix-ups", "Manufacturing material removal", "Material/Component Contamination", "No Marketing Application", "Package design/selection
", "Packaging", "Packaging change control", "Packaging process control", "Release of Material/Component prior to receiving test results", "Reprocessing Controls", "Storage")

data <- labeled %>% 
  # filter out FDA cause categories according to paper
  filter(!(FDAcause %in% fda_causes_rm)) %>% 
  mutate(error = if_else(UI == 1 | CTRL == 1 | SW == 1, 1, 0)) %>% 
  # select cause and recall text
  select(RECALL_NUMBER, MANUFACTURER_RECALL_REASON, FDAcause, error)

train_data <- data %>% 
  filter(str_sub(RECALL_NUMBER, -4, -1) == 2012 | str_sub(RECALL_NUMBER, -4, -1) == 2013 | str_sub(RECALL_NUMBER, -4, -1) == 2014)

test_data <- data %>% 
  filter(str_sub(RECALL_NUMBER, -4, -1) == 2015)

get_grams <- function(df){

# unigram
unigram_counts <- df %>% 
  unnest_tokens(word, MANUFACTURER_RECALL_REASON) %>% 
  anti_join(stop_words, by = "word") %>% 
  # mutate(word = wordStem(word)) %>%
  count(RECALL_NUMBER, word, sort = TRUE)

# bigram 
bigram_counts <- df %>% 
  unnest_tokens(word, MANUFACTURER_RECALL_REASON, token = "ngrams", n = 2) %>% 
  separate(word, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  unite(word, word1, word2, sep = " ") %>% 
  count(RECALL_NUMBER, word, sort = TRUE)

# trigram
trigram_counts <- df %>% 
  unnest_tokens(word, MANUFACTURER_RECALL_REASON, token = "ngrams", n = 3) %>% 
  separate(word, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word3 %in% stop_words$word) %>% 
  unite(word, word1, word2, word3, sep = " ") %>% 
  count(RECALL_NUMBER, word, sort = TRUE)

# count words using unigram and bigram method
data_counts <- bind_rows(unigram_counts, bigram_counts, trigram_counts) %>% 
  arrange(desc(n))

return(data_counts)

}

train_data_counts <- get_grams(train_data)
test_data_counts <- get_grams(test_data)


get_dtm <- function(df) {
  
# document term matrix using tf_idf statistic
data_dtm <- df %>% 
  bind_tf_idf(word, RECALL_NUMBER, n) %>% 
  cast_dtm(RECALL_NUMBER, word, n)

return(data_dtm)
}

train_dtm <- get_dtm(train_data_counts)
test_dtm <- get_dtm(test_data_counts)
```

```{r MODEL, message=FALSE, warning=FALSE, include=FALSE, cache=TRUE}
# fits LDA model with k=2 topics on test data
lda_fit <- LDA(train_dtm, k = 2, method = "GIBBS", control = list(seed = 1234))
```

```{r PLOTS, message=FALSE, warning=FALSE, include=FALSE}
# get predicted probabilities of each topic for each recall document
lda_pred <- posterior(lda_fit, test_dtm)[["topics"]] %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "RECALL_NUMBER") %>% 
  mutate(topic = if_else(`1` > `2`, 1, 2)) %>% 
  inner_join(test_data, by = "RECALL_NUMBER") %>% 
  select(RECALL_NUMBER, `1`, `2`, topic, error)

lda_pred_df <- posterior(lda_fit, test_dtm)[["topics"]] %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "RECALL_NUMBER") %>% 
  mutate(topic = if_else(`1` > `2`, 1, 2)) %>% 
  head(10)

# confusion matrix for test data
lda_confusion_matrix <- table(lda_pred$topic, lda_pred$error)

# looking at true errors predicted as non-errors
lda_error_fp <- lda_pred %>% 
  filter(topic==1 & error==1) %>% 
  inner_join(test_data)

# look at a few non-errors classified as errors
lda_nonerror_fp <- lda_pred %>% 
  filter(topic==2 & error==0) %>% 
  inner_join(test_data) %>% 
  top_n(5)

# probabilities each word belongs to one of k=20 topics
lda_word_probs <- tidy(lda_fit, matrix = "beta")

# estimated probability each document belongs to each topic
lda_cause_probs <- tidy(lda_fit, matrix = "gamma")

################################
# top 10 words within each topic
lda_topwords <- lda_word_probs %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta) %>% 
  ggplot() + 
  aes(term, beta, fill = factor(topic)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 3)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip() +
  theme_hc()

################################
# dist. of topics
lda_topic_dist <- inner_join(lda_cause_probs, train_data, by = c(document = "RECALL_NUMBER")) %>% 
  select(document, topic, gamma, error) %>% 
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() + 
  facet_wrap(~error) +
  theme_hc()
```

### Introduction

Natural language processing can be very useful in analyzing and modelling text data. In our project we were tasked with developing a model that successfully predicts user-interface(UI) software errors and software errors as classified manually in Zhang et al.[@zhang] using text data from the FDA. We were given data from years 2012-2015 as well as the labelled results from Zhang et al.[@zhang]. In total, there were 7,771 recalls with 423 due to software UI errors. Each recall was given an FDA determined cause along with a manufacturer recall reason among many other variables. The goal is to use the manufacturer recall reason text data in order to train a NLP model to predict these recalls labelled software UI errors.

### Methods

We decided to use a topic modelling approach to solving this problem, and in particular used Latent Dirichlet Allocation(LDA)[@blei2003latent]-more on this later-using the `topicmodels`[@R-topicmodels] R package. Prior to fitting the model there was some data pre-processing to take care of. First, we merged the FDA manufacturer recall text data with the labelled results from Zhang et al.[@zhang]. Second, we created a new error label. Each of the 7,771 recalls used in the paper above was labelled as a non-error or any combination of the following three errors: UI-SW(user-interface software error), CTRL-SW(control software error), and SW(software). We merged these three categories into one, that is, if a recall was labelled any of the three we labelled it an error otherwise a non-error. Next, we filtered out recalls based on the FDA determined cause as the paper above. Lastly we removed all stop words and considered all unigrams, bigrams and trigrams in form of a document term matrix. We then trained an LDA model with data from years 2012-2014 and used year 2015's data to assess our model. 

### Model

LDA is an unsupervised topic modelling algorithm that identifies a given number of topics that best generate a text corpus.  A topic is a list of words along with a distribution over those words (i.e. some words will appear more frequently than others within a topic).  Training an LDA model on a corpus produces two collections of data: one containing the identified topics, and another containing the mix of topics that comprises each document in the corpus.  Table 1 below shows topics identified in the FDA dataset.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(lda_pred_df[1:7, ], "latex", digits = 2, booktabs = T, caption = "Document/Topic Probabilities") %>% 
  kable_styling(latex_options = c("striped", "hold_position"), full_width = F, font_size = 8) %>% 
  add_header_above(c(" ", "Predicted Topic Probabilities" = 2, "Predicted Topic" = 1))
```

To gain an intuitive understanding of LDA, it’s useful to explore the generative process it assumes.  LDA assumes that documents are built by randomly drawing the appropriate number of words from the documents’ topics.  For example, a three-topic, 100-word document comprised of 50% technology, 30% business, and 20% arts would be made up of 50 words randomly drawn from the technology topic, 30 from the business topic, and 20 from the arts topic.  LDA is a bag-of-words model; word order and syntax are not considered.  Therefore, any document generated by this process would not be human-readable, but its topics could likely still be discerned.  LDA executes this process in reverse; starting with a corpus of documents, the topics and document compositions are determined.  

Because LDA is an unsupervised algorithm these topics will not be named, but it’s sometimes possible to infer what a topic is from its most common words.  For example, in the figure below it’s easy to see that topic 10 refers to incidents of sterile conditions being lost due to breaches in packaging.  Other topics, for example topic 11, are less obvious. 

```{r echo=FALSE, out.width="90%"}
knitr::include_graphics("20Topics_6_15.png")
```

LDA may be tuned with two parameters; alpha and beta.  The value of alpha specifies how similar or dissimilar documents in the corpus are to one another, while beta has a similar effect on the topics.  Lower values of alpha and beta indicate less similarity.  For the task of classification, low alpha values are desirable because it causes documents to be more heavily represented by a single topic.

```{r echo=FALSE, out.width="90%"}
knitr::include_graphics("dirichlet_plot.png")
```

The effect of varying the alpha parameter is shown in the figure above[@alphaFig].  Topics are represented by the corners of the triangle.  Lower alpha values drive the documents to a single topic or a mix of two, while higher values produce a more even mix of all three topics.

```{r echo=FALSE, out.width="90%"}
knitr::include_graphics("graphical.png")
```

Above is a graphical model representation of LDA.  M represents documents, and N represents the assignment of topics (z) to words (w).  Theta represents the topic distribution of the document, which is influenced by the alpha parameter. 

\pagebreak

### Results

Using the LDA model with 2 topics we trained using data from years 2012-2015. An advantage of using LDA is that it's a probabilistic method. That is, we define the underlying data belongs to 2 topics and the model calculates the probability that each recall belongs to one of the two topics. Along with these probabilities, it also calculates the probability that each word(or bigram/trigram) belongs to each topic. This is useful because we can visualize the top words belonging to each topic as well as the the distribution of document topic probabilities by their being labelled an error or non-error. 

```{r echo=FALSE, fig.asp=.618, out.height="50%"}
lda_topwords + lda_topic_dist
```

The two plots shown above are the top 10 words that occur in each topic-1 and 2-assigned by the trained LDA model(left) and the distribution of each recall(right) belonging to each topic-1 and 2-split by the recall being labelled an error(1) or non-error(0). 

Now, with LDA being an unsupervised method we must interpret the topics given by the LDA model, namely, topics 1 and 2. From the word/topic plot on the left we can see that the model is picking up some interesting words belonging to topic 2, notably, the words systems, system, software, and potential, which corresponds to our task of classifying a recall as an error. However, the word "recalling" holds a high probability of occuring in topic 1 whereas the word "recall" is given a high probability of occuring in topic 2. This can be troublesome due to the two words likely meaning the same thing in the context of labelling a recall as an error or non-error. The document/topic plot on the right also reveals some information in interpreting topics. Most notably, recalls that are labelled errors(1) have a higher probability of occuring in topic 2 of the LDA model, whereas, recalls labelled non-errors(0) have a higher probability of occuring in topic 1. From this we will define topic 2 as a recall being classified an error and topic 1 being classified a non-error.

\pagebreak

### Validation 

Now that we have a model trained and topics defined we can use it to make predictions on our test data from year 2015. The confusion matrix is shown below. There are a total of 1,879 recalls in our test data with 122 having been labelled an error. 

```{r echo=FALSE}
kable(lda_confusion_matrix, "latex", row.names = TRUE, booktabs = T, caption = "Confusion Matrix") %>% 
  kable_styling(latex_options = c("striped", "hold_position"), full_width = FALSE, font_size = 8) %>% 
  add_header_above(c(" ", "No-error"=1, "Error"=1)) %>% 
  add_footnote(c("ACC.: 46.5%", "TPR: 95.9%"), notation = "symbol")
```

Overall accuracy of the model(46.5%) is not that great but due to class-imbalance we included the true positive rate as an alternative evaluation metric. The true positive rate is 95.9%, that is, it was able to correctly identify approximately 96% of recalls labelled errors. However, there are 5 false negatives(true errors labelled non-error-topic 1) and 999 false positives(true non-errors labeled error-topic 2). The false negatives(Table 3) and a few false positives(Table 4) are shown below.  

```{r echo=FALSE}
temp1 <- lda_error_fp %>% select(2:6)

kable(temp1, "latex", digits = 2, caption = "False Negatives", booktabs = T) %>% 
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 8) %>% 
  add_header_above(c("Topic Probabilities" = 2, "Predicted"=1, "Actual"=1, " ")) %>% 
  column_spec(5, width = "30em")
```

```{r echo=FALSE}
temp2 <- lda_nonerror_fp %>% select(2:6)

kable(temp2[1:4,], "latex", digits = 2, caption = "False Positives", booktabs = T) %>% 
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 8) %>% 
  add_header_above(c("Topic Probabilities" = 2, "Predicted"=1, "Actual"=1, " ")) %>% 
  column_spec(5, width = "30em")
  
```


There are a few things to note from the tables listed above:

1. Reading the text of each recall, it's very difficult to discern some as being labelled an error or not.

2. Most of the incorrectly classified recalls have probabilities of belonging to topic 1 and topic 2 very close to one another. We used the default method for classifying a recall. That is, a recall is classified to the topic having the largest probability of belonging to that topic. This could potentially be modified to eliminate false negatives and improve the true positive rate, assuming an error labelled a non-error is more costly than a non-error labelled an error.

3. There are many words(other than stop words) that seem to have no importance in classifying a recall an error or non-error but adds probability to belonging to topic 1 or 2 by sheer abundance. 

### Conclusion

Using Latent Dirichlet Allocation we were able to correctly identify ~96% of recalls labelled being an error. However, due to the similarity between error and non-error text data used, overall accuracy was hindered. One solution to this would be to manually set the classification rule based on topic probabilities to maximize the true positive rate whilst limiting the false negative rate. At the very least, this method has the potential to reduce the amount of recalls classified manually significantly and could very well be fine tuned to eliminate nonsignificant words used interchangeably between recalls labelled error and non-error.

### Acknowledgements

This project was made exponentially easier thanks to the following packages: `tidyverse`[@R-tidyverse], `tidytext`[@R-tidytext], `kableExtra`[@R-kableExtra], `broom`[@R-broom], `topicmodels`[@R-topicmodels], `readxl`[@R-readxl], `tm`[@R-tm], `ggthemes`[@R-ggthemes], and `patchwork`[@R-patchwork].